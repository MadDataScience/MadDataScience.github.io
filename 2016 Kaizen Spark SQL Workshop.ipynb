{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark SQL and Joining Distributed Data Frames\n",
    "\n",
    "**Alessandro Gagliardi**  \n",
    "_Lead Professor_  \n",
    "[GalvanizeU](http://www.galvanizeu.com/) powered by [University of New Haven](http://galvanizeu.newhaven.edu/)  \n",
    "Twitter: [`@MadDataScience`](https://twitter.com/MadDataScience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "1. A (very) brief history of SQL and the relational model\n",
    "2. Scaling and NoSQL\n",
    "3. Spark SQL to the rescue\n",
    "4. The Trouble with Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### You are expected to already:\n",
    "\n",
    "1. Be familiar with the basics of SQL including JOIN\n",
    "2. Be familiar with the basics of Apache Spark including RDDs and Map and Reduce operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### By the end of this workshop, you should be able to:\n",
    "\n",
    "1. Place SQL in its historical context with regard to databases and distributed architectures\n",
    "2. Explain the difference between NoSQL and NOSQL\n",
    "3. Identify which JOIN method to use on distributed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DB Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 1960s - B.C. (Before Codd)\n",
    "\n",
    "- Hierarchical data structure (IBM IMS)\n",
    "- Network data structure (CODASYL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1970s - The Birth of the Relational Model\n",
    "- [_A Relational Model of Data for Large Shared Data Banks_](https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf) by E.F. Codd (1970)\n",
    "- Relational Model: Relations, Tuples, and Attributes\n",
    "- Structured Query Language (SQL): Tables, Rows, and Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1980s - Commercialization of RDBMS\n",
    "- [_Principles of Transaction-Oriented Database Recovery_](https://web.stanford.edu/class/cs340v/papers/recovery.pdf) by Haerder & Reuter (1983)\n",
    "- Atomicity, Consistency, Isolation, Durability (ACID)\n",
    "- Oracle, IBM DB2, Sybase, Microsoft SQL Server_etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1990s - PC and Open Source RDBMS\n",
    "- [_The Third Manifesto_](http://www09.sigmod.org/sigmod/record/issues/9503/manifesto.ps) by Darwen & Date (1995)\n",
    "- MySQL and PostgreSQL\n",
    "- LAMP (Linux, Apache, MySQL, PHP) Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2000s - Begining of Big Data\n",
    "- [_Towards Robust Distributed Systems_](https://people.eecs.berkeley.edu/~brewer/cs262b-2004/PODC-keynote.pdf) by Eric Brewer (2000)\n",
    "- Consistency, Availability, and Partition tolerance (CAP)\n",
    "- Google File System, Bigtable, MapReduce, _etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2010s - Open Source Big Data\n",
    "\n",
    "- Distributed Data Analysis - Hadoop, Spark, Storm, _etc._\n",
    "- NoSQL (or more appropriately 'NoREL') - Cassandra, Mongo, Riak, _etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Relational Model in SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### CRUD\n",
    "\n",
    "- **C**reate - **INSERT**\n",
    "- **R**ead   - **SELECT**\n",
    "- **U**pdate - **UPDATE**\n",
    "- **D**elete - **DELETE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Operations on Relations\n",
    "\n",
    "1. Permutation\n",
    "2. Projection\n",
    "3. Join\n",
    "4. Composition\n",
    "5. Restriction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### SQL Example: \n",
    "```sql\n",
    "  \n",
    "SELECT customers.name, -- permuation and projection\n",
    "     visits.created_at \n",
    "FROM visits JOIN customers -- join and composition\n",
    "     USING (customer_id)    \n",
    "WHERE customer_id = 1  -- restriction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Which operation do you think is the most challenging to implement in a distributed architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Q: Are joins slow? <br /> A: It depends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Join Algorithms\n",
    "\n",
    "- Nested Loop Join\n",
    "  - Pseudocode\n",
    "  \n",
    "  ```\n",
    "  For each tuple r in R do\n",
    "     For each tuple s in S do\n",
    "        If r and s satisfy the join condition\n",
    "           Then output the tuple <r,s>\n",
    "           \n",
    "  ```  \n",
    "  - Time: `O(R*S)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Sort-Merge Join\n",
    "  - If sorted: `O(R+S)`\n",
    "  - If not sorted: `O(R*log(R)+S*log(S))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Hash Join\n",
    "  1. Build hash table (_i.e._ key-value store) of smaller relation in memory\n",
    "  2. Scan larger relation for relevant rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NoSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NoSQL means NoREL  \n",
    "NoREL means NoJOIN\n",
    "\n",
    "[CQL (Cassandra Query Language) example](https://docs.datastax.com/en/cql/3.0/cql/cql_reference/select_r.html):\n",
    "```sql\n",
    "SELECT select_expression\n",
    "FROM keyspace_name.table_name\n",
    "WHERE relation AND relation ... \n",
    "ORDER BY ( clustering_column ( ASC | DESC )...)\n",
    "LIMIT n\n",
    "```\n",
    "\n",
    "Q: What do you do if you can't join?  \n",
    "A: Denormalize!  \n",
    "Q: What's wrong with denormalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "{u'contributors': None,\n",
    " u'coordinates': None,\n",
    " u'created_at': u'Fri Jan 23 04:23:34 +0000 2015',\n",
    " u'entities': {u'hashtags': [{u'indices': [84, 95], u'text': u'500kLubaTV'}],\n",
    "  u'media': [{u'display_url': u'pic.twitter.com/1zhgdIaxsl',\n",
    "    u'expanded_url': u'http://twitter.com/LubaTV/status/558474724968529921/photo/1',\n",
    "    u'id': 558474718849007616L,\n",
    "    u'id_str': u'558474718849007616',\n",
    "    u'indices': [96, 118],\n",
    "    u'media_url': u'http://pbs.twimg.com/media/B8AZzEEIAAAFGY0.jpg',\n",
    "    u'media_url_https': u'https://pbs.twimg.com/media/B8AZzEEIAAAFGY0.jpg',\n",
    "    u'sizes': {u'large': {u'h': 270, u'resize': u'fit', u'w': 750},\n",
    "     u'medium': {u'h': 216, u'resize': u'fit', u'w': 600},\n",
    "     u'small': {u'h': 122, u'resize': u'fit', u'w': 340},\n",
    "     u'thumb': {u'h': 150, u'resize': u'crop', u'w': 150}},\n",
    "    u'source_status_id': 558474724968529921L,\n",
    "    u'source_status_id_str': u'558474724968529921',\n",
    "    u'source_user_id': 121517065,\n",
    "    u'source_user_id_str': u'121517065',\n",
    "    u'type': u'photo',\n",
    "    u'url': u'http://t.co/1zhgdIaxsl'}],\n",
    "  u'symbols': [],\n",
    "  u'urls': [],\n",
    "  u'user_mentions': [{u'id': 121517065,\n",
    "    u'id_str': u'121517065',\n",
    "    u'indices': [3, 10],\n",
    "    u'name': u'Lucas Feuersch\\xfctte',\n",
    "    u'screen_name': u'LubaTV'}]},\n",
    " u'favorite_count': 0,\n",
    " u'favorited': False,\n",
    " u'geo': None,\n",
    " u'id': 558480131778691072L,\n",
    " u'id_str': u'558480131778691072',\n",
    " u'in_reply_to_screen_name': None,\n",
    " u'in_reply_to_status_id': None,\n",
    " u'in_reply_to_status_id_str': None,\n",
    " u'in_reply_to_user_id': None,\n",
    " u'in_reply_to_user_id_str': None,\n",
    " u'lang': u'pt',\n",
    " u'metadata': {u'iso_language_code': u'pt', u'result_type': u'recent'},\n",
    " u'place': None,\n",
    " u'possibly_sensitive': False,\n",
    " u'retweet_count': 395,\n",
    " u'retweeted': False,\n",
    " u'retweeted_status': {u'contributors': None,\n",
    "  u'coordinates': None,\n",
    "  u'created_at': u'Fri Jan 23 04:02:05 +0000 2015',\n",
    "  u'entities': {u'hashtags': [{u'indices': [72, 83], u'text': u'500kLubaTV'}],\n",
    "   u'media': [{u'display_url': u'pic.twitter.com/1zhgdIaxsl',\n",
    "     u'expanded_url': u'http://twitter.com/LubaTV/status/558474724968529921/photo/1',\n",
    "     u'id': 558474718849007616L,\n",
    "     u'id_str': u'558474718849007616',\n",
    "     u'indices': [84, 106],\n",
    "     u'media_url': u'http://pbs.twimg.com/media/B8AZzEEIAAAFGY0.jpg',\n",
    "     u'media_url_https': u'https://pbs.twimg.com/media/B8AZzEEIAAAFGY0.jpg',\n",
    "     u'sizes': {u'large': {u'h': 270, u'resize': u'fit', u'w': 750},\n",
    "      u'medium': {u'h': 216, u'resize': u'fit', u'w': 600},\n",
    "      u'small': {u'h': 122, u'resize': u'fit', u'w': 340},\n",
    "      u'thumb': {u'h': 150, u'resize': u'crop', u'w': 150}},\n",
    "     u'type': u'photo',\n",
    "     u'url': u'http://t.co/1zhgdIaxsl'}],\n",
    "   u'symbols': [],\n",
    "   u'urls': [],\n",
    "   u'user_mentions': []},\n",
    "  u'favorite_count': 901,\n",
    "  u'favorited': False,\n",
    "  u'geo': None,\n",
    "  u'id': 558474724968529921L,\n",
    "  u'id_str': u'558474724968529921',\n",
    "  u'in_reply_to_screen_name': None,\n",
    "  u'in_reply_to_status_id': None,\n",
    "  u'in_reply_to_status_id_str': None,\n",
    "  u'in_reply_to_user_id': None,\n",
    "  u'in_reply_to_user_id_str': None,\n",
    "  u'lang': u'pt',\n",
    "  u'metadata': {u'iso_language_code': u'pt', u'result_type': u'recent'},\n",
    "  u'place': None,\n",
    "  u'possibly_sensitive': False,\n",
    "  u'retweet_count': 395,\n",
    "  u'retweeted': False,\n",
    "  u'source': u'<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
    "  u'text': u\"\\xc9 oficial, turma. Somos 500 mil agora! Obrigado, obrigado por tudo. =') #500kLubaTV http://t.co/1zhgdIaxsl\",\n",
    "  u'truncated': False,\n",
    "  u'user': {u'contributors_enabled': False,\n",
    "   u'created_at': u'Tue Mar 09 19:04:06 +0000 2010',\n",
    "   u'default_profile': False,\n",
    "   u'default_profile_image': False,\n",
    "   u'description': u'Sou da turma! \\u270c\\ufe0f Contato: lubatv3@gmail.com | Instagram & Facebook: LubaTV | Caixa Postal 174, 88701-970 Tubar\\xe3o - SC | \\xdaltimo v\\xeddeo: \\u2b07\\ufe0f',\n",
    "   u'entities': {u'description': {u'urls': []},\n",
    "    u'url': {u'urls': [{u'display_url': u'youtu.be/ntMBRQYs-hI',\n",
    "       u'expanded_url': u'http://youtu.be/ntMBRQYs-hI',\n",
    "       u'indices': [0, 22],\n",
    "       u'url': u'http://t.co/4n2e3pI9To'}]}},\n",
    "   u'favourites_count': 722,\n",
    "   u'follow_request_sent': False,\n",
    "   u'followers_count': 118167,\n",
    "   u'following': False,\n",
    "   u'friends_count': 548,\n",
    "   u'geo_enabled': True,\n",
    "   u'id': 121517065,\n",
    "   u'id_str': u'121517065',\n",
    "   u'is_translation_enabled': False,\n",
    "   u'is_translator': False,\n",
    "   u'lang': u'pt',\n",
    "   u'listed_count': 261,\n",
    "   u'location': u'',\n",
    "   u'name': u'Lucas Feuersch\\xfctte',\n",
    "   u'notifications': False,\n",
    "   u'profile_background_color': u'FFF04D',\n",
    "   u'profile_background_image_url': u'http://pbs.twimg.com/profile_background_images/520656124547067904/of-yDc4j.jpeg',\n",
    "   u'profile_background_image_url_https': u'https://pbs.twimg.com/profile_background_images/520656124547067904/of-yDc4j.jpeg',\n",
    "   u'profile_background_tile': True,\n",
    "   u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/121517065/1419199713',\n",
    "   u'profile_image_url': u'http://pbs.twimg.com/profile_images/553302094132690944/cEZfRo8r_normal.png',\n",
    "   u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/553302094132690944/cEZfRo8r_normal.png',\n",
    "   u'profile_link_color': u'0099CC',\n",
    "   u'profile_location': None,\n",
    "   u'profile_sidebar_border_color': u'FFFFFF',\n",
    "   u'profile_sidebar_fill_color': u'DDEEF6',\n",
    "   u'profile_text_color': u'333333',\n",
    "   u'profile_use_background_image': True,\n",
    "   u'protected': False,\n",
    "   u'screen_name': u'LubaTV',\n",
    "   u'statuses_count': 36586,\n",
    "   u'time_zone': u'Brasilia',\n",
    "   u'url': u'http://t.co/4n2e3pI9To',\n",
    "   u'utc_offset': -7200,\n",
    "   u'verified': False}},\n",
    " u'source': u'<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
    " u'text': u\"RT @LubaTV: \\xc9 oficial, turma. Somos 500 mil agora! Obrigado, obrigado por tudo. =') #500kLubaTV http://t.co/1zhgdIaxsl\",\n",
    " u'truncated': False,\n",
    " u'user': {u'contributors_enabled': False,\n",
    "  u'created_at': u'Thu Dec 20 20:15:56 +0000 2012',\n",
    "  u'default_profile': False,\n",
    "  u'default_profile_image': False,\n",
    "  u'description': u'F\\xe3 de uns carinhas ai do YouTube \\u2665 1/1/15 - Rafa respondeu *u*                       19/01/15 - Nay Respondeu *u* #Forfun\\xe1tica',\n",
    "  u'entities': {u'description': {u'urls': []}},\n",
    "  u'favourites_count': 596,\n",
    "  u'follow_request_sent': False,\n",
    "  u'followers_count': 36,\n",
    "  u'following': False,\n",
    "  u'friends_count': 94,\n",
    "  u'geo_enabled': False,\n",
    "  u'id': 1024945802,\n",
    "  u'id_str': u'1024945802',\n",
    "  u'is_translation_enabled': False,\n",
    "  u'is_translator': False,\n",
    "  u'lang': u'pt',\n",
    "  u'listed_count': 1,\n",
    "  u'location': u'Rio de Janeiro',\n",
    "  u'name': u'Biah #ADR ',\n",
    "  u'notifications': False,\n",
    "  u'profile_background_color': u'ACDED6',\n",
    "  u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme18/bg.gif',\n",
    "  u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme18/bg.gif',\n",
    "  u'profile_background_tile': False,\n",
    "  u'profile_banner_url': u'https://pbs.twimg.com/profile_banners/1024945802/1421771885',\n",
    "  u'profile_image_url': u'http://pbs.twimg.com/profile_images/556923388615090176/UiQb8z54_normal.jpeg',\n",
    "  u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/556923388615090176/UiQb8z54_normal.jpeg',\n",
    "  u'profile_link_color': u'9266CC',\n",
    "  u'profile_location': None,\n",
    "  u'profile_sidebar_border_color': u'FFFFFF',\n",
    "  u'profile_sidebar_fill_color': u'F6F6F6',\n",
    "  u'profile_text_color': u'333333',\n",
    "  u'profile_use_background_image': True,\n",
    "  u'protected': False,\n",
    "  u'screen_name': u'_beatriz_27',\n",
    "  u'statuses_count': 838,\n",
    "  u'time_zone': u'Brasilia',\n",
    "  u'url': None,\n",
    "  u'utc_offset': -7200,\n",
    "  u'verified': False}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(to be fair, normalized data can be quite complex too)\n",
    "\n",
    "Q: What's the *real* problem with denormalization?  \n",
    "A: It limits the kinds of questions you can ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark SQL is not an RDBMS\n",
    "\n",
    "Rather (like Hive) merely translates SQL queries into jobs that can be run against a distributed file system\n",
    "* Hive translates HQL into Hadoop MapReduce jobs\n",
    "* Spark SQL translates SQL into Spark a DAG of transformations (including `map()` and `reduce()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark SQL employs SchemaRDDs, more specifically Spark DataFrames.\n",
    "\n",
    "What is a DataFrame?\n",
    "\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "\n",
    "- Think of a DataFrames as RDDs with schema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is a schema?\n",
    "\n",
    "- Schemas are metadata about your data.\n",
    "\n",
    "- Schemas define table names, column names, and column types over your\n",
    "  data.\n",
    "\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs,\n",
    "  instead of using column positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark SQL Using CSV\n",
    "-------------------\n",
    "\n",
    "To proceed with the demo, please go to [http://tinyurl.com/sparksql16](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6780843632664124/3482466428365889/2932358616588278/latest.html) and import the notebook into your own [Databricks](https://community.cloud.databricks.com) cloud.\n",
    "\n",
    "How can I pull in my CSV data and use Spark SQL on it?\n",
    "\n",
    "- Make sure the CSV exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sales.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.csv\n",
    "#ID,Date,Store,State,Product,Amount\n",
    "101,11/13/2014,100,WA,331,300.00\n",
    "104,11/18/2014,700,OR,329,450.00\n",
    "102,11/15/2014,203,CA,321,200.00\n",
    "106,11/19/2014,202,CA,331,330.00\n",
    "103,11/17/2014,101,WA,373,750.00\n",
    "105,11/19/2014,202,CA,321,200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Read the file and convert columns to right types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, u'11/13/2014', 100, u'WA', 331, 300.0),\n",
       " (104, u'11/18/2014', 700, u'OR', 329, 450.0),\n",
       " (102, u'11/15/2014', 203, u'CA', 321, 200.0),\n",
       " (106, u'11/19/2014', 202, u'CA', 331, 330.0),\n",
       " (103, u'11/17/2014', 101, u'WA', 373, 750.0),\n",
       " (105, u'11/19/2014', 202, u'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('sales.csv')\\\n",
    "    .filter(lambda line: not line.startswith('#'))\\\n",
    "    .map(lambda line: line.split(','))\\\n",
    "    .map(lambda \\\n",
    "      (id,date,store,state,product,amount):\\\n",
    "      (int(id),date,int(store),state,int(product),float(amount)))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Import data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Define a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType( [\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('store',IntegerType(),True),\n",
    "    StructField('state',StringType(),True),\n",
    "    StructField('product',IntegerType(),True),\n",
    "    StructField('amount',FloatType(),True) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Define the DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!--\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What change do we have to make to the code above if we are\n",
    "processing a TSV file instead of a CSV file?\n",
    "</summary>\n",
    "<br>\n",
    "Replace `line.split(',')` with `line.split('\\t')`\n",
    "</details>\n",
    "-->\n",
    "Using SQL With DataFrames\n",
    "-------------------------\n",
    "\n",
    "How can I run SQL queries on DataFrames?\n",
    "\n",
    "- Register the table with SqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Run queries on the registered tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state|amount|\n",
      "+-----+------+\n",
      "|   WA| 300.0|\n",
      "|   OR| 450.0|\n",
      "|   CA| 200.0|\n",
      "|   CA| 330.0|\n",
      "|   WA| 750.0|\n",
      "|   CA| 200.0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('SELECT state,amount from sales where amount > 100').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **The Trouble with Joins**\n",
    "\n",
    "There are certain times when you may have to customize joins performed by Spark. One of those options is the `BroadcastHashJoin` while the other is the `ShuffledHashJoin`.\n",
    "* Enabling `BroadcastHashJoin` can optimize joining a large and a small table in Spark SQL.\n",
    "* This notebook will cover the how to configure a `BroadcastHashJoin` and why to choose it over a `ShuffledHashJoin`.\n",
    "\n",
    "The following is adapted from [this presentation](http://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Join a Large Table with a Small Table\n",
    "\n",
    "```sql\n",
    "SELECT * \n",
    "  FROM people_in_the_us \n",
    "  JOIN states\n",
    "  ON people_in_the_us.state = states.name\n",
    "```  \n",
    "<BR />  \n",
    "  \n",
    "* **`ShuffledHashJoin?`**\n",
    "* **`BroadcastHashJoin?`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ShuffledHashJoin](http://image.slidesharecdn.com/stratasj-everydayimshuffling-tipsforwritingbettersparkprograms-150223113317-conversion-gate02/95/everyday-im-shuffling-tips-for-writing-better-spark-programs-strata-san-jose-2015-13-638.jpg?cb=1427111079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![BroadcastHashJoin](http://image.slidesharecdn.com/stratasj-everydayimshuffling-tipsforwritingbettersparkprograms-150223113317-conversion-gate02/95/everyday-im-shuffling-tips-for-writing-better-spark-programs-strata-san-jose-2015-14-638.jpg?cb=1427111079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Join a Medium Table with a Huge Table\n",
    "```sql\n",
    "SELECT *\n",
    "  FROM people_in_california\n",
    "  LEFT JOIN all_the_people_in_the_world\n",
    "  ON people_in_california.id = all_the_people_in_the_world.id\n",
    "```  \n",
    "  \n",
    "**Final output keys = keys `people_in_california`, so this doesn't need a huge Spark cluster, right?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Left Join - Shuffle Step](http://image.slidesharecdn.com/stratasj-everydayimshuffling-tipsforwritingbettersparkprograms-150223113317-conversion-gate02/95/everyday-im-shuffling-tips-for-writing-better-spark-programs-strata-san-jose-2015-17-638.jpg?cb=1427111079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![What's a Better Solution?](http://image.slidesharecdn.com/stratasj-everydayimshuffling-tipsforwritingbettersparkprograms-150223113317-conversion-gate02/95/everyday-im-shuffling-tips-for-writing-better-spark-programs-strata-san-jose-2015-18-638.jpg?cb=1427111079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Practice:** Create a large table that will be joined with a smaller table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "array = []\n",
    "for i in range(0, 1000000):\n",
    "  array.append(Row(num=i, bit = i % 2))\n",
    "  \n",
    "dataFrame = sqlContext.createDataFrame(sc.parallelize(array))\n",
    "dataFrame.repartition(100).registerTempTable(\"my_large_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|bit|num|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  0|  2|\n",
      "|  1|  3|\n",
      "|  0|  4|\n",
      "|  1|  5|\n",
      "|  0|  6|\n",
      "|  1|  7|\n",
      "|  0|  8|\n",
      "|  1|  9|\n",
      "|  0| 10|\n",
      "|  1| 11|\n",
      "|  0| 12|\n",
      "|  1| 13|\n",
      "|  0| 14|\n",
      "|  1| 15|\n",
      "|  0| 16|\n",
      "|  1| 17|\n",
      "|  0| 18|\n",
      "|  1| 19|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### By default, Spark will not use BroadcastHashJoin to join this table with a small table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "array = []\n",
    "for i in range(0, 2):\n",
    "  array.append(Row(bit=i))\n",
    "  \n",
    "dataFrame = sqlContext.createDataFrame(sc.parallelize(array))\n",
    "dataFrame.registerTempTable(\"my_small_temp_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tip: `EXPLAIN` can be used to print out the Spark execution plan for a Spark SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*SortMergeJoin [bit#29L], [bit#40L], Inner\n",
      ":- *Sort [bit#29L ASC], false, 0\n",
      ":  +- Exchange hashpartitioning(bit#29L, 200)\n",
      ":     +- Exchange RoundRobinPartitioning(100)\n",
      ":        +- *Filter isnotnull(bit#29L)\n",
      ":           +- Scan ExistingRDD[bit#29L,num#30L]\n",
      "+- *Sort [bit#40L ASC], false, 0\n",
      "   +- Exchange hashpartitioning(bit#40L, 200)\n",
      "      +- *Filter isnotnull(bit#40L)\n",
      "         +- Scan ExistingRDD[bit#40L]\n"
     ]
    }
   ],
   "source": [
    "e = sqlContext.sql('''EXPLAIN SELECT * \n",
    "    FROM my_large_table \n",
    "    JOIN my_small_temp_table \n",
    "    ON my_large_table.bit = my_small_temp_table.bit''').collect()\n",
    "print(e[0]['plan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **ShuffleHashJoin** and **SortMergeJoin** are not a great ways to join the two tables above.\n",
    "*  It will take all the rows in `my_large_table` and shuffle them with the \"bit\" key.\n",
    "* **NOTE:** There will only be **2** non-empty partitions for the whole table, and adding more worker nodes to the job would not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 500000), (69, 500000)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output_index_and_count(index, iter):\n",
    "  count = 0\n",
    "  for item in iter:\n",
    "    count += 1\n",
    "  yield (index, count)\n",
    "\n",
    "sqlContext.sql(\"\"\"SELECT * \n",
    "    FROM my_large_table \n",
    "    JOIN my_small_temp_table \n",
    "    ON my_large_table.bit = my_small_temp_table.bit\"\"\")\\\n",
    "  .rdd.mapPartitionsWithIndex(output_index_and_count)\\\n",
    "      .filter(lambda p: p[1] > 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **BroadcastHashJoin** is a much better way to join these two tables.\n",
    "* SparkSQL detects that one table is small enough to broadcast.\n",
    "* The small table is broadcast to each Spark worker and joined with each element in the larger RDD.\n",
    "* There will be more than 2 non-empty partitions, so Spark would run faster with more than two worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to use `BroadcastHashJoin`, save the table with DataFrames API. In that way spark will automatically figure out that the table is small enough to be broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql('DROP TABLE IF EXISTS my_small_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "array = []\n",
    "for i in range(0, 2):\n",
    "  array.append(Row(bit=i))\n",
    "  \n",
    "dataFrame = sqlContext.createDataFrame(sc.parallelize(array))\n",
    "dataFrame.write.saveAsTable(\"my_small_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use an `EXPLAIN` command to see the BroadcastHashJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*BroadcastHashJoin [bit#29L], [bit#64L], Inner, BuildRight\n",
      ":- Exchange RoundRobinPartitioning(100)\n",
      ":  +- *Filter isnotnull(bit#29L)\n",
      ":     +- Scan ExistingRDD[bit#29L,num#30L]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n",
      "   +- *Project [bit#64L]\n",
      "      +- *Filter isnotnull(bit#64L)\n",
      "         +- *BatchedScan parquet default.my_small_table[bit#64L] Format: ParquetFormat, InputPaths: file:/Users/alessandro/Dropbox/MadDataScience.github.io/spark-warehouse/my_small_table, PushedFilters: [IsNotNull(bit)], ReadSchema: struct<bit:bigint>\n"
     ]
    }
   ],
   "source": [
    "e = sqlContext.sql('''EXPLAIN SELECT * \n",
    "    FROM my_large_table \n",
    "    JOIN my_small_table \n",
    "    ON my_large_table.bit = my_small_table.bit''').collect()\n",
    "print(e[0]['plan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you didn't see a `BroadcastHashJoin` - uncomment and run the following cell:\n",
    "* Prior to Spark 1.4, you must manually run an analyze on the table.\n",
    "* With Spark 1.4 or greater - tables created with Hive DDL's rather than the Dataframes API may also need an analyze command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following command and run it if you did not see a BroadcastHashJoin.\n",
    "# sqlContext.sql('ANALYZE TABLE my_small_table COMPUTE STATISTICS noscan').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Join of the tables with BroadcastHashJoin\n",
    "\n",
    "See how there are now 100 evenly split partitions in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "broadcastHashJoinRdd = sqlContext.sql(\"\"\"SELECT * \n",
    "    FROM my_large_table \n",
    "    JOIN my_small_table \n",
    "    ON my_large_table.bit = my_small_table.bit\"\"\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 10000),\n",
       " (1, 10000),\n",
       " (2, 10000),\n",
       " (3, 10000),\n",
       " (4, 10000),\n",
       " (5, 10000),\n",
       " (6, 10000),\n",
       " (7, 10000),\n",
       " (8, 10000),\n",
       " (9, 10001),\n",
       " (10, 10001),\n",
       " (11, 10001),\n",
       " (12, 10001),\n",
       " (13, 10001),\n",
       " (14, 10001),\n",
       " (15, 10001),\n",
       " (16, 10001),\n",
       " (17, 10000),\n",
       " (18, 10000),\n",
       " (19, 10000),\n",
       " (20, 10000),\n",
       " (21, 10000),\n",
       " (22, 10000),\n",
       " (23, 10000),\n",
       " (24, 10000),\n",
       " (25, 10000),\n",
       " (26, 10000),\n",
       " (27, 10000),\n",
       " (28, 10000),\n",
       " (29, 10000),\n",
       " (30, 10000),\n",
       " (31, 10000),\n",
       " (32, 10000),\n",
       " (33, 10000),\n",
       " (34, 10000),\n",
       " (35, 10001),\n",
       " (36, 10001),\n",
       " (37, 10001),\n",
       " (38, 10001),\n",
       " (39, 10001),\n",
       " (40, 10001),\n",
       " (41, 10001),\n",
       " (42, 10000),\n",
       " (43, 10000),\n",
       " (44, 10000),\n",
       " (45, 10000),\n",
       " (46, 10000),\n",
       " (47, 10000),\n",
       " (48, 10000),\n",
       " (49, 10000),\n",
       " (50, 10000),\n",
       " (51, 10000),\n",
       " (52, 10000),\n",
       " (53, 10000),\n",
       " (54, 10000),\n",
       " (55, 10000),\n",
       " (56, 10000),\n",
       " (57, 10000),\n",
       " (58, 10000),\n",
       " (59, 10000),\n",
       " (60, 10000),\n",
       " (61, 10001),\n",
       " (62, 10001),\n",
       " (63, 10001),\n",
       " (64, 10001),\n",
       " (65, 10000),\n",
       " (66, 10000),\n",
       " (67, 9999),\n",
       " (68, 9999),\n",
       " (69, 9999),\n",
       " (70, 9999),\n",
       " (71, 9999),\n",
       " (72, 9999),\n",
       " (73, 9999),\n",
       " (74, 9999),\n",
       " (75, 9999),\n",
       " (76, 9999),\n",
       " (77, 9999),\n",
       " (78, 9999),\n",
       " (79, 9999),\n",
       " (80, 9999),\n",
       " (81, 9999),\n",
       " (82, 9999),\n",
       " (83, 9999),\n",
       " (84, 9999),\n",
       " (85, 9999),\n",
       " (86, 10000),\n",
       " (87, 10000),\n",
       " (88, 10000),\n",
       " (89, 10000),\n",
       " (90, 10000),\n",
       " (91, 10000),\n",
       " (92, 10000),\n",
       " (93, 10000),\n",
       " (94, 10000),\n",
       " (95, 10000),\n",
       " (96, 10000),\n",
       " (97, 10000),\n",
       " (98, 10000),\n",
       " (99, 10000)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastHashJoinRdd.mapPartitionsWithIndex(output_index_and_count).filter(lambda p: p[1] > 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Cleanup: delete any tables that were created for this example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql('DROP TABLE IF EXISTS my_small_table')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:dsci6007]",
   "language": "python",
   "name": "conda-env-dsci6007-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "workshop lab",
  "notebookId": 3767766597858743
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
